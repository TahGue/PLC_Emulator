{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eb43599",
   "metadata": {},
   "source": [
    "# LSTM Autoencoder for ICS Anomaly Detection\\n\\n**Industrial Control System (ICS) Time-Series Anomaly Detection Pipeline**\\n\\nThis notebook demonstrates a complete ML/DL pipeline for detecting cyberattacks on industrial control systems using an LSTM Autoencoder trained on PLC telemetry data.\\n\\n## Pipeline Overview\\n1. **Data Generation** — Synthetic normal + attack telemetry from PLC simulation\\n2. **Preprocessing** — Feature scaling, sliding window sequences\\n3. **Model Architecture** — LSTM Encoder-Decoder autoencoder\\n4. **Training** — Reconstruct normal operation patterns\\n5. **Evaluation** — Detection rates across 8 ICS attack types\\n6. **Visualization** — ROC curves, confusion matrices, feature importance\\n7. **Real-time Inference** — API integration demo\\n\\n### Feature Vector (14 dimensions per timestep)\\n| # | Feature | Type | Description |\\n|---|---------|------|-------------|\\n| 0 | conveyor_running | bool | Main conveyor motor state |\\n| 1 | production_rate | float | Bottles/min throughput |\\n| 2 | reject_rate | float | Reject percentage |\\n| 3 | in_flight_bottles | int | Bottles in transit |\\n| 4-6 | bottle_at_* | bool | Station presence sensors |\\n| 7-8 | output_* | bool | PLC alarm/reject outputs |\\n| 9 | network_packet_rate | float | Modbus/TCP packets per second |\\n| 10 | network_burst_ratio | float | Traffic burst ratio (0-1) |\\n| 11 | scan_time_ms | float | PLC scan cycle time |\\n| 12-13 | io_*_sum | int | Total active I/O counts |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40814111",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f7e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(os.path.join('..', 'backend')))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
    "\n",
    "# Dark theme matching the PLC Emulator UI\n",
    "matplotlib.rcParams.update({\n",
    "    'figure.facecolor': '#0f172a', 'axes.facecolor': '#1e293b',\n",
    "    'text.color': '#e2e8f0', 'axes.labelcolor': '#94a3b8',\n",
    "    'xtick.color': '#64748b', 'ytick.color': '#64748b',\n",
    "    'axes.edgecolor': '#334155', 'grid.color': '#334155', 'grid.alpha': 0.5,\n",
    "})\n",
    "\n",
    "from app.ml.lstm_autoencoder import (\n",
    "    generate_normal_data, generate_attack_data,\n",
    "    train_lstm_autoencoder, create_sequences,\n",
    "    FeatureScaler, LSTMAutoencoder, FEATURE_NAMES, N_FEATURES,\n",
    "    load_lstm_artifact, save_lstm_artifact,\n",
    ")\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Features: {N_FEATURES} — {', '.join(FEATURE_NAMES[:7])}...\")\n",
    "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9de4aa",
   "metadata": {},
   "source": [
    "## 2. Data Generation & Exploration\n",
    "\n",
    "Generate synthetic telemetry for normal operation and 8 ICS attack scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8945ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate normal baseline data\n",
    "normal_data = generate_normal_data(n_samples=8000, seed=42)\n",
    "print(f\"Normal data shape: {normal_data.shape}\")\n",
    "\n",
    "# Generate attack data for each type\n",
    "ATTACK_TYPES = [\n",
    "    \"dos_flood\", \"mitm\", \"false_data_injection\", \"modbus_injection\",\n",
    "    \"stuxnet_like\", \"sensor_jamming\", \"replay\", \"combined\",\n",
    "]\n",
    "attack_datasets = {}\n",
    "for at in ATTACK_TYPES:\n",
    "    attack_datasets[at] = generate_attack_data(n_samples=1000, attack_type=at)\n",
    "    print(f\"  {at}: {attack_datasets[at].shape}\")\n",
    "\n",
    "# Visualize feature distributions: Normal vs DoS vs MITM\n",
    "fig, axes = plt.subplots(3, 5, figsize=(18, 8))\n",
    "fig.suptitle(\"Feature Distributions: Normal vs Attack\", fontsize=14, color='#a78bfa')\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i >= N_FEATURES:\n",
    "        ax.set_visible(False)\n",
    "        continue\n",
    "    ax.hist(normal_data[:, i], bins=30, alpha=0.6, color='#22c55e', label='Normal', density=True)\n",
    "    ax.hist(attack_datasets['dos_flood'][:, i], bins=30, alpha=0.5, color='#ef4444', label='DoS', density=True)\n",
    "    ax.hist(attack_datasets['mitm'][:, i], bins=30, alpha=0.4, color='#f59e0b', label='MITM', density=True)\n",
    "    ax.set_title(FEATURE_NAMES[i], fontsize=7, color='#94a3b8')\n",
    "    ax.tick_params(labelsize=6)\n",
    "    if i == 0:\n",
    "        ax.legend(fontsize=6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ade8da",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "\n",
    "Train the LSTM Autoencoder on normal-operation data only. The model learns to reconstruct normal patterns — high reconstruction error signals anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad06d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LSTM Autoencoder\n",
    "artifact = train_lstm_autoencoder(\n",
    "    normal_data,\n",
    "    seq_len=30, hidden_dim=64, latent_dim=16, n_layers=2,\n",
    "    epochs=50, batch_size=64, learning_rate=1e-3,\n",
    "    threshold_quantile=0.98, verbose=True,\n",
    ")\n",
    "\n",
    "# Plot training loss curve\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(artifact['epoch_losses'], color='#a78bfa', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss Curve', color='#a78bfa')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "meta = artifact['metadata']\n",
    "print(f\"\\nModel: {meta['model_version']}\")\n",
    "print(f\"Threshold: {meta['threshold']:.6f}\")\n",
    "print(f\"Final loss: {meta['final_train_loss']:.6f}\")\n",
    "print(f\"Train time: {meta['train_time_seconds']}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98ca116",
   "metadata": {},
   "source": [
    "## 4. Evaluation — Detection Rates & ROC Curves\n",
    "\n",
    "Evaluate the trained model against 8 ICS/SCADA attack types and compute per-attack detection rates, ROC curves, and a multi-class confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8c0483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for evaluation\n",
    "scaler = FeatureScaler.from_dict(artifact['scaler'])\n",
    "metadata = artifact['metadata']\n",
    "seq_len = metadata['seq_len']\n",
    "threshold = metadata['threshold']\n",
    "\n",
    "model = LSTMAutoencoder(\n",
    "    input_dim=N_FEATURES, hidden_dim=metadata['hidden_dim'],\n",
    "    latent_dim=metadata['latent_dim'], seq_len=seq_len, n_layers=metadata['n_layers'],\n",
    ")\n",
    "model.load_state_dict(artifact['state_dict'])\n",
    "model.eval()\n",
    "\n",
    "def compute_errors(data):\n",
    "    scaled = scaler.transform(data)\n",
    "    seqs = create_sequences(scaled, seq_len)\n",
    "    tensor = torch.from_numpy(seqs)\n",
    "    with torch.no_grad():\n",
    "        recon = model(tensor)\n",
    "        return torch.mean((recon - tensor) ** 2, dim=(1, 2)).cpu().numpy()\n",
    "\n",
    "# Normal baseline errors\n",
    "normal_errors = compute_errors(generate_normal_data(1000, seed=999))\n",
    "\n",
    "# Per-attack errors + ROC data\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 8))\n",
    "fig.suptitle(\"ROC Curves per Attack Type\", fontsize=14, color='#a78bfa')\n",
    "colors = ['#ef4444','#f59e0b','#22c55e','#3b82f6','#a78bfa','#ec4899','#06b6d4','#f97316']\n",
    "\n",
    "all_y_true, all_y_scores = [], []\n",
    "detection_rates = {}\n",
    "\n",
    "for idx, (attack, color) in enumerate(zip(ATTACK_TYPES, colors)):\n",
    "    attack_errors = compute_errors(attack_datasets[attack])\n",
    "    \n",
    "    y_true = np.concatenate([np.zeros(len(normal_errors)), np.ones(len(attack_errors))])\n",
    "    y_scores = np.concatenate([normal_errors, attack_errors])\n",
    "    all_y_true.extend(y_true)\n",
    "    all_y_scores.extend(y_scores)\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    det_rate = (attack_errors >= threshold).sum() / len(attack_errors) * 100\n",
    "    detection_rates[attack] = det_rate\n",
    "    \n",
    "    ax = axes[idx // 4][idx % 4]\n",
    "    ax.plot(fpr, tpr, color=color, linewidth=2, label=f'AUC={roc_auc:.3f}')\n",
    "    ax.plot([0,1], [0,1], '--', color='#475569', linewidth=0.8)\n",
    "    ax.fill_between(fpr, tpr, alpha=0.15, color=color)\n",
    "    ax.set_title(f\"{attack} ({det_rate:.0f}%)\", fontsize=9, color='#94a3b8')\n",
    "    ax.legend(fontsize=8, loc='lower right')\n",
    "    ax.set_xlim(0, 1); ax.set_ylim(0, 1.02)\n",
    "    ax.tick_params(labelsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "bars = ax.barh(list(detection_rates.keys()), list(detection_rates.values()), color=colors)\n",
    "ax.set_xlabel('Detection Rate (%)')\n",
    "ax.set_title('Attack Detection Rates', color='#a78bfa')\n",
    "ax.set_xlim(0, 105)\n",
    "for bar, rate in zip(bars, detection_rates.values()):\n",
    "    ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, f'{rate:.1f}%', \n",
    "            va='center', fontsize=9, color='#e2e8f0')\n",
    "ax.axvline(x=90, color='#22c55e', linestyle='--', alpha=0.5, label='90% target')\n",
    "ax.legend(fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "avg_det = np.mean(list(detection_rates.values()))\n",
    "fpr_normal = (normal_errors >= threshold).sum() / len(normal_errors) * 100\n",
    "print(f\"\\nAverage Detection Rate: {avg_det:.1f}%\")\n",
    "print(f\"False Positive Rate: {fpr_normal:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb9c6c0",
   "metadata": {},
   "source": [
    "## 5. Feature Importance & Error Analysis\n",
    "\n",
    "Analyze which features contribute most to anomaly detection for each attack type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e5c522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-feature error heatmap across attack types\n",
    "feature_error_matrix = np.zeros((len(ATTACK_TYPES), N_FEATURES))\n",
    "\n",
    "for i, attack in enumerate(ATTACK_TYPES):\n",
    "    data = attack_datasets[attack]\n",
    "    scaled = scaler.transform(data)\n",
    "    seqs = create_sequences(scaled, seq_len)\n",
    "    tensor = torch.from_numpy(seqs)\n",
    "    with torch.no_grad():\n",
    "        recon = model(tensor)\n",
    "        per_feat = torch.mean((recon - tensor) ** 2, dim=(0, 1)).cpu().numpy()\n",
    "        feature_error_matrix[i] = per_feat\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "im = ax.imshow(feature_error_matrix, aspect='auto', cmap='magma')\n",
    "ax.set_xticks(range(N_FEATURES))\n",
    "ax.set_xticklabels([f.replace('_', '\\n') for f in FEATURE_NAMES], fontsize=7, rotation=45, ha='right')\n",
    "ax.set_yticks(range(len(ATTACK_TYPES)))\n",
    "ax.set_yticklabels(ATTACK_TYPES, fontsize=9)\n",
    "ax.set_title('Per-Feature Reconstruction Error by Attack Type', color='#a78bfa', fontsize=13)\n",
    "plt.colorbar(im, ax=ax, label='MSE')\n",
    "\n",
    "# Annotate cells\n",
    "for i in range(len(ATTACK_TYPES)):\n",
    "    for j in range(N_FEATURES):\n",
    "        val = feature_error_matrix[i, j]\n",
    "        color = '#e2e8f0' if val < feature_error_matrix.max() * 0.5 else '#0f172a'\n",
    "        ax.text(j, i, f'{val:.3f}', ha='center', va='center', fontsize=6, color=color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec03f99",
   "metadata": {},
   "source": [
    "## 6. Confusion Matrix & Classification Report\n",
    "\n",
    "Binary classification: Normal (0) vs Anomaly (1) using the learned threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e2084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all attack errors for binary classification\n",
    "all_y_true = np.array(all_y_true)\n",
    "all_y_scores = np.array(all_y_scores)\n",
    "all_y_pred = (all_y_scores >= threshold).astype(int)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(all_y_true, all_y_pred)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot confusion matrix\n",
    "im = ax1.imshow(cm, cmap='Blues')\n",
    "ax1.set_xticks([0, 1]); ax1.set_yticks([0, 1])\n",
    "ax1.set_xticklabels(['Normal', 'Anomaly'])\n",
    "ax1.set_yticklabels(['Normal', 'Anomaly'])\n",
    "ax1.set_xlabel('Predicted'); ax1.set_ylabel('Actual')\n",
    "ax1.set_title('Confusion Matrix', color='#a78bfa')\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        ax1.text(j, i, f'{cm[i,j]}', ha='center', va='center', fontsize=16, \n",
    "                color='#0f172a' if cm[i,j] > cm.max()/2 else '#e2e8f0')\n",
    "\n",
    "# Overall ROC\n",
    "fpr_all, tpr_all, _ = roc_curve(all_y_true, all_y_scores)\n",
    "roc_auc_all = auc(fpr_all, tpr_all)\n",
    "ax2.plot(fpr_all, tpr_all, color='#a78bfa', linewidth=2.5, label=f'Overall AUC = {roc_auc_all:.4f}')\n",
    "ax2.plot([0,1], [0,1], '--', color='#475569')\n",
    "ax2.fill_between(fpr_all, tpr_all, alpha=0.15, color='#a78bfa')\n",
    "ax2.set_xlabel('False Positive Rate'); ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title('Overall ROC Curve', color='#a78bfa')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.set_xlim(0, 1); ax2.set_ylim(0, 1.02)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(all_y_true, all_y_pred, target_names=['Normal', 'Anomaly']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c01571",
   "metadata": {},
   "source": [
    "## 7. Model Architecture Summary\n",
    "\n",
    "Inspect the LSTM Autoencoder architecture and parameter count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701966a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture summary\n",
    "print(\"LSTM Autoencoder Architecture\")\n",
    "print(\"=\" * 50)\n",
    "print(model)\n",
    "print(\"=\" * 50)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters:     {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size:           {total_params * 4 / 1024:.1f} KB (float32)\")\n",
    "print(f\"\\nHyperparameters:\")\n",
    "for k in ['seq_len','hidden_dim','latent_dim','n_layers','epochs','batch_size','learning_rate']:\n",
    "    print(f\"  {k}: {metadata[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc467d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8. Save & Deploy\n",
    "\n",
    "Save the trained artifact for use with the FastAPI backend inference endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156fae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model artifact\n",
    "output_path = os.path.join('..', 'backend', 'models', 'lstm_anomaly_detector.pt')\n",
    "save_lstm_artifact(artifact, output_path)\n",
    "\n",
    "import pathlib\n",
    "size_kb = pathlib.Path(output_path).stat().st_size / 1024\n",
    "print(f\"Artifact saved to: {output_path}\")\n",
    "print(f\"File size: {size_kb:.0f} KB\")\n",
    "print(f\"\\nTo use with the backend API:\")\n",
    "print(f\"  export LSTM_MODEL_PATH=/app/models/lstm_anomaly_detector.pt\")\n",
    "print(f\"  # or set in docker-compose.yml environment\")\n",
    "print(f\"\\nAPI endpoints:\")\n",
    "print(f\"  POST /anomaly/score   — ingest + score telemetry\")\n",
    "print(f\"  POST /anomaly/ingest  — buffer telemetry only\")\n",
    "print(f\"  GET  /anomaly/status  — model status\")\n",
    "print(f\"  GET  /anomaly/history — score history\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
